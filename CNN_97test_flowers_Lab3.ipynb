{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cGTQg8suRDT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "731a8f55-0d82-4efe-8bf8-3f33b88fb4be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Settings\n",
            "batch_size:  16 \n",
            "learning_rate: 0.0001 \n",
            "max epochs:  300 \n",
            "momentum:  0.9\n",
            "\n",
            "Train Epoch: 1  \n",
            "Loss: 1.629074 \n",
            "Training Accuracy: 0.537202\n",
            "test accuracy:  0.5821917808219178\n",
            "\n",
            "Train Epoch: 2  \n",
            "Loss: 0.826294 \n",
            "Training Accuracy: 0.715030\n",
            "test accuracy:  0.730593607305936\n",
            "\n",
            "Train Epoch: 3  \n",
            "Loss: 0.783544 \n",
            "Training Accuracy: 0.699405\n",
            "test accuracy:  0.7602739726027398\n",
            "\n",
            "Train Epoch: 4  \n",
            "Loss: 0.760442 \n",
            "Training Accuracy: 0.702381\n",
            "test accuracy:  0.773972602739726\n",
            "\n",
            "Train Epoch: 5  \n",
            "Loss: 0.616206 \n",
            "Training Accuracy: 0.773065\n",
            "test accuracy:  0.7625570776255708\n",
            "\n",
            "Train Epoch: 6  \n",
            "Loss: 0.793439 \n",
            "Training Accuracy: 0.732143\n",
            "test accuracy:  0.7442922374429224\n",
            "\n",
            "Train Epoch: 7  \n",
            "Loss: 0.733317 \n",
            "Training Accuracy: 0.723958\n",
            "test accuracy:  0.7488584474885844\n",
            "\n",
            "Train Epoch: 8  \n",
            "Loss: 0.552504 \n",
            "Training Accuracy: 0.790179\n",
            "test accuracy:  0.8127853881278538\n",
            "\n",
            "Train Epoch: 9  \n",
            "Loss: 0.533620 \n",
            "Training Accuracy: 0.787946\n",
            "test accuracy:  0.821917808219178\n",
            "\n",
            "Train Epoch: 10  \n",
            "Loss: 0.555624 \n",
            "Training Accuracy: 0.788690\n",
            "test accuracy:  0.815068493150685\n",
            "\n",
            "Train Epoch: 11  \n",
            "Loss: 0.554017 \n",
            "Training Accuracy: 0.775298\n",
            "test accuracy:  0.8127853881278538\n",
            "\n",
            "Train Epoch: 12  \n",
            "Loss: 0.535834 \n",
            "Training Accuracy: 0.802827\n",
            "test accuracy:  0.819634703196347\n",
            "\n",
            "Train Epoch: 13  \n",
            "Loss: 0.462597 \n",
            "Training Accuracy: 0.836310\n",
            "test accuracy:  0.8310502283105022\n",
            "\n",
            "Train Epoch: 14  \n",
            "Loss: 0.462106 \n",
            "Training Accuracy: 0.817708\n",
            "test accuracy:  0.8310502283105022\n",
            "\n",
            "Train Epoch: 15  \n",
            "Loss: 0.372642 \n",
            "Training Accuracy: 0.870536\n",
            "test accuracy:  0.8744292237442922\n",
            "\n",
            "Train Epoch: 16  \n",
            "Loss: 0.433431 \n",
            "Training Accuracy: 0.849702\n",
            "test accuracy:  0.865296803652968\n",
            "\n",
            "Train Epoch: 17  \n",
            "Loss: 0.384872 \n",
            "Training Accuracy: 0.848214\n",
            "test accuracy:  0.8105022831050228\n",
            "\n",
            "Train Epoch: 18  \n",
            "Loss: 0.524685 \n",
            "Training Accuracy: 0.808036\n",
            "test accuracy:  0.8493150684931506\n",
            "\n",
            "Train Epoch: 19  \n",
            "Loss: 0.409867 \n",
            "Training Accuracy: 0.848214\n",
            "test accuracy:  0.8287671232876712\n",
            "\n",
            "Train Epoch: 20  \n",
            "Loss: 0.412697 \n",
            "Training Accuracy: 0.850446\n",
            "test accuracy:  0.8515981735159818\n",
            "\n",
            "Train Epoch: 21  \n",
            "Loss: 0.433035 \n",
            "Training Accuracy: 0.835565\n",
            "test accuracy:  0.865296803652968\n",
            "\n",
            "Train Epoch: 22  \n",
            "Loss: 0.350823 \n",
            "Training Accuracy: 0.854167\n",
            "test accuracy:  0.8767123287671232\n",
            "\n",
            "Train Epoch: 23  \n",
            "Loss: 0.376960 \n",
            "Training Accuracy: 0.851190\n",
            "test accuracy:  0.8378995433789954\n",
            "\n",
            "Train Epoch: 24  \n",
            "Loss: 0.391242 \n",
            "Training Accuracy: 0.843750\n",
            "test accuracy:  0.8538812785388128\n",
            "\n",
            "Train Epoch: 25  \n",
            "Loss: 0.350922 \n",
            "Training Accuracy: 0.866071\n",
            "test accuracy:  0.867579908675799\n",
            "\n",
            "Train Epoch: 26  \n",
            "Loss: 0.404942 \n",
            "Training Accuracy: 0.841518\n",
            "test accuracy:  0.8584474885844748\n",
            "\n",
            "Train Epoch: 27  \n",
            "Loss: 0.315890 \n",
            "Training Accuracy: 0.886161\n",
            "test accuracy:  0.8881278538812786\n",
            "\n",
            "Train Epoch: 28  \n",
            "Loss: 0.345910 \n",
            "Training Accuracy: 0.863839\n",
            "test accuracy:  0.867579908675799\n",
            "\n",
            "Train Epoch: 29  \n",
            "Loss: 0.355262 \n",
            "Training Accuracy: 0.877976\n",
            "test accuracy:  0.8698630136986302\n",
            "\n",
            "Train Epoch: 30  \n",
            "Loss: 0.346508 \n",
            "Training Accuracy: 0.862351\n",
            "test accuracy:  0.867579908675799\n",
            "\n",
            "Train Epoch: 31  \n",
            "Loss: 0.328768 \n",
            "Training Accuracy: 0.867560\n",
            "test accuracy:  0.8789954337899544\n",
            "\n",
            "Train Epoch: 32  \n",
            "Loss: 0.274170 \n",
            "Training Accuracy: 0.901786\n",
            "test accuracy:  0.8789954337899544\n",
            "\n",
            "Train Epoch: 33  \n",
            "Loss: 0.387023 \n",
            "Training Accuracy: 0.864583\n",
            "test accuracy:  0.8881278538812786\n",
            "\n",
            "Train Epoch: 34  \n",
            "Loss: 0.366176 \n",
            "Training Accuracy: 0.844494\n",
            "test accuracy:  0.8858447488584474\n",
            "\n",
            "Train Epoch: 35  \n",
            "Loss: 0.343024 \n",
            "Training Accuracy: 0.883929\n",
            "test accuracy:  0.8972602739726028\n",
            "\n",
            "Train Epoch: 36  \n",
            "Loss: 0.266464 \n",
            "Training Accuracy: 0.898065\n",
            "test accuracy:  0.8858447488584474\n",
            "\n",
            "Train Epoch: 37  \n",
            "Loss: 0.365258 \n",
            "Training Accuracy: 0.859375\n",
            "test accuracy:  0.8926940639269406\n",
            "\n",
            "Train Epoch: 38  \n",
            "Loss: 0.282711 \n",
            "Training Accuracy: 0.900298\n",
            "test accuracy:  0.8721461187214612\n",
            "\n",
            "Train Epoch: 39  \n",
            "Loss: 0.312059 \n",
            "Training Accuracy: 0.884673\n",
            "test accuracy:  0.8721461187214612\n",
            "\n",
            "Train Epoch: 40  \n",
            "Loss: 0.337031 \n",
            "Training Accuracy: 0.877976\n",
            "test accuracy:  0.8904109589041096\n",
            "\n",
            "Train Epoch: 41  \n",
            "Loss: 0.293261 \n",
            "Training Accuracy: 0.890625\n",
            "test accuracy:  0.9018264840182648\n",
            "\n",
            "Train Epoch: 42  \n",
            "Loss: 0.282006 \n",
            "Training Accuracy: 0.902530\n",
            "test accuracy:  0.8835616438356164\n",
            "\n",
            "Train Epoch: 43  \n",
            "Loss: 0.379144 \n",
            "Training Accuracy: 0.842262\n",
            "test accuracy:  0.8858447488584474\n",
            "\n",
            "Train Epoch: 44  \n",
            "Loss: 0.313096 \n",
            "Training Accuracy: 0.880208\n",
            "test accuracy:  0.8858447488584474\n",
            "\n",
            "Train Epoch: 45  \n",
            "Loss: 0.320265 \n",
            "Training Accuracy: 0.886161\n",
            "test accuracy:  0.9041095890410958\n",
            "\n",
            "Train Epoch: 46  \n",
            "Loss: 0.276017 \n",
            "Training Accuracy: 0.919643\n",
            "test accuracy:  0.8789954337899544\n",
            "\n",
            "Train Epoch: 47  \n",
            "Loss: 0.277854 \n",
            "Training Accuracy: 0.906250\n",
            "test accuracy:  0.91324200913242\n",
            "\n",
            "Train Epoch: 48  \n",
            "Loss: 0.306235 \n",
            "Training Accuracy: 0.901786\n",
            "test accuracy:  0.908675799086758\n",
            "\n",
            "Train Epoch: 49  \n",
            "Loss: 0.301676 \n",
            "Training Accuracy: 0.889881\n",
            "test accuracy:  0.9269406392694064\n",
            "\n",
            "Train Epoch: 50  \n",
            "Loss: 0.301927 \n",
            "Training Accuracy: 0.893601\n",
            "test accuracy:  0.9155251141552512\n",
            "\n",
            "Train Epoch: 51  \n",
            "Loss: 0.275611 \n",
            "Training Accuracy: 0.885417\n",
            "test accuracy:  0.9315068493150684\n",
            "\n",
            "Train Epoch: 52  \n",
            "Loss: 0.296182 \n",
            "Training Accuracy: 0.899554\n",
            "test accuracy:  0.8698630136986302\n",
            "\n",
            "Train Epoch: 53  \n",
            "Loss: 0.236072 \n",
            "Training Accuracy: 0.921875\n",
            "test accuracy:  0.9018264840182648\n",
            "\n",
            "Train Epoch: 54  \n",
            "Loss: 0.253601 \n",
            "Training Accuracy: 0.921875\n",
            "test accuracy:  0.9200913242009132\n",
            "\n",
            "Train Epoch: 55  \n",
            "Loss: 0.274958 \n",
            "Training Accuracy: 0.892857\n",
            "test accuracy:  0.9041095890410958\n",
            "\n",
            "Train Epoch: 56  \n",
            "Loss: 0.227563 \n",
            "Training Accuracy: 0.913690\n",
            "test accuracy:  0.8698630136986302\n",
            "\n",
            "Train Epoch: 57  \n",
            "Loss: 0.272833 \n",
            "Training Accuracy: 0.911458\n",
            "test accuracy:  0.9200913242009132\n",
            "\n",
            "Train Epoch: 58  \n",
            "Loss: 0.261481 \n",
            "Training Accuracy: 0.906250\n",
            "test accuracy:  0.9200913242009132\n",
            "\n",
            "Train Epoch: 59  \n",
            "Loss: 0.223632 \n",
            "Training Accuracy: 0.920387\n",
            "test accuracy:  0.908675799086758\n",
            "\n",
            "Train Epoch: 60  \n",
            "Loss: 0.227598 \n",
            "Training Accuracy: 0.912946\n",
            "test accuracy:  0.910958904109589\n",
            "\n",
            "Train Epoch: 61  \n",
            "Loss: 0.271502 \n",
            "Training Accuracy: 0.924107\n",
            "test accuracy:  0.9200913242009132\n",
            "\n",
            "Train Epoch: 62  \n",
            "Loss: 0.263933 \n",
            "Training Accuracy: 0.912946\n",
            "test accuracy:  0.9223744292237442\n",
            "\n",
            "Train Epoch: 63  \n",
            "Loss: 0.210213 \n",
            "Training Accuracy: 0.919643\n",
            "test accuracy:  0.9041095890410958\n",
            "\n",
            "Train Epoch: 64  \n",
            "Loss: 0.243787 \n",
            "Training Accuracy: 0.912946\n",
            "test accuracy:  0.9041095890410958\n",
            "\n",
            "Train Epoch: 65  \n",
            "Loss: 0.253568 \n",
            "Training Accuracy: 0.915923\n",
            "test accuracy:  0.9246575342465754\n",
            "\n",
            "Train Epoch: 66  \n",
            "Loss: 0.239903 \n",
            "Training Accuracy: 0.909226\n",
            "test accuracy:  0.9337899543378996\n",
            "\n",
            "Train Epoch: 67  \n",
            "Loss: 0.215909 \n",
            "Training Accuracy: 0.928571\n",
            "test accuracy:  0.9200913242009132\n",
            "\n",
            "Train Epoch: 68  \n",
            "Loss: 0.219879 \n",
            "Training Accuracy: 0.933036\n",
            "test accuracy:  0.9474885844748858\n",
            "\n",
            "Train Epoch: 69  \n",
            "Loss: 0.185709 \n",
            "Training Accuracy: 0.935268\n",
            "test accuracy:  0.9269406392694064\n",
            "\n",
            "Train Epoch: 70  \n",
            "Loss: 0.240665 \n",
            "Training Accuracy: 0.910714\n",
            "test accuracy:  0.91324200913242\n",
            "\n",
            "Train Epoch: 71  \n",
            "Loss: 0.238964 \n",
            "Training Accuracy: 0.901042\n",
            "test accuracy:  0.9337899543378996\n",
            "\n",
            "Train Epoch: 72  \n",
            "Loss: 0.260907 \n",
            "Training Accuracy: 0.892857\n",
            "test accuracy:  0.9429223744292238\n",
            "\n",
            "Train Epoch: 73  \n",
            "Loss: 0.271356 \n",
            "Training Accuracy: 0.900298\n",
            "test accuracy:  0.9200913242009132\n",
            "\n",
            "Train Epoch: 74  \n",
            "Loss: 0.195622 \n",
            "Training Accuracy: 0.919643\n",
            "test accuracy:  0.9200913242009132\n",
            "\n",
            "Train Epoch: 75  \n",
            "Loss: 0.272254 \n",
            "Training Accuracy: 0.898810\n",
            "test accuracy:  0.9269406392694064\n",
            "\n",
            "Train Epoch: 76  \n",
            "Loss: 0.232794 \n",
            "Training Accuracy: 0.919643\n",
            "test accuracy:  0.9452054794520548\n",
            "\n",
            "Train Epoch: 77  \n",
            "Loss: 0.250604 \n",
            "Training Accuracy: 0.912946\n",
            "test accuracy:  0.9383561643835616\n",
            "\n",
            "Train Epoch: 78  \n",
            "Loss: 0.214726 \n",
            "Training Accuracy: 0.928571\n",
            "test accuracy:  0.9246575342465754\n",
            "\n",
            "Train Epoch: 79  \n",
            "Loss: 0.184751 \n",
            "Training Accuracy: 0.946429\n",
            "test accuracy:  0.9429223744292238\n",
            "\n",
            "Train Epoch: 80  \n",
            "Loss: 0.217532 \n",
            "Training Accuracy: 0.918155\n",
            "test accuracy:  0.9315068493150684\n",
            "\n",
            "Train Epoch: 81  \n",
            "Loss: 0.236798 \n",
            "Training Accuracy: 0.926339\n",
            "test accuracy:  0.9360730593607306\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on:  06/22/22\n",
        "author:      Ian Dwyer\n",
        "institution: University of Colorado Denver\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" ----------------------- IMPORT LIBRARIES ---------------------- \"\"\"\n",
        "\n",
        "##### 1. Import libraries\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\"\"\" --------------------------- GLOBALS --------------------------- \"\"\"\n",
        "\n",
        "\n",
        "#### 3. Define the model\n",
        "#### defines the model as a custom nn.Module subclass\n",
        "\n",
        "class MLP(nn.Module):                                                                 # must call nn.module subclass\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()                                               # super() calls parent class\n",
        "\n",
        "        self.model = nn.Sequential(                                               # sequential() connect the layers\n",
        "            nn.Linear(size, neurons),                                             # linear reg at each node\n",
        "            nn.LeakyReLU(inplace=True),                                           # activation function\n",
        "            nn.Linear(neurons, neurons*4),\n",
        "            nn.LeakyReLU(inplace=True),                                           # activation function\n",
        "            nn.Linear(neurons*4, neurons),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.Linear(neurons, outputs),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):                                                         # pushes values through model\n",
        "        x = self.model(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class CNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "      super(CNN,self).__init__()\n",
        "                                                              # 3 convolutional layers\n",
        "      self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3,32,kernel_size=3,padding=1),          #no loss in img dimension due to padding\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)   #1/4 of the feats go in, size passed to next layer 32*(224/2*224/2)=224*224*8\n",
        "        )\n",
        "      self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32,64,kernel_size=3,padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)   #1/4 of the feats go in, size passed to next layer 64*(224/4*224/4)=224*224*4\n",
        "        )\n",
        "      self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(64,256,kernel_size=3,padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2) #1/4 of the feats go in,  size passed to next layer 256*(224/8*224/8)=224*224*4\n",
        "        )\n",
        "\n",
        "      self.fc = nn.Linear(256*28*28,outputs)        # 1 fully connected layer (256 28*28 images due to 3 layers of pooling)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "                                                                                  #flatten before fully connecting convolutional layers\n",
        "        x = x.view(x.size(0),-1)                                                  ##reshape in test and train is rendundant but not necessary to remove\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\"\"\" -------------------------- FUNCTIONS -------------------------- \"\"\"\n",
        "\n",
        "#### 4. Prepare the data\n",
        "\n",
        "def CIFAR10_train():\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([transforms.ToTensor(),\n",
        "                   transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
        "                   ), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return train_loader\n",
        "\n",
        "def CIFAR10_test():\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('../data', train=False, download=True,\n",
        "                   transform=transforms.Compose([transforms.ToTensor(),\n",
        "                   transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
        "                  ),batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return test_loader\n",
        "\n",
        "def transform(train_dir,test_dir,rotation,resize,norm_param):\n",
        "  train_transforms = transforms.Compose([transforms.RandomRotation(rotation),\n",
        "                                        transforms.RandomResizedCrop(resize),\n",
        "                                        transforms.RandomHorizontalFlip(),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize( norm_param[0],norm_param[1] )]\n",
        "                                      )\n",
        "\n",
        "  test_transforms = transforms.Compose([transforms.Resize(rotation),\n",
        "                                        transforms.CenterCrop(resize),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize( norm_param[0],norm_param[1])]\n",
        "                                      )\n",
        "  train_data = datasets.ImageFolder(train_dir,transform = train_transforms)\n",
        "  test_data = datasets.ImageFolder(train_dir,transform = train_transforms)\n",
        "\n",
        "  return train_data,test_data\n",
        "\n",
        "\n",
        "#### 6. Start training\n",
        "\n",
        "def train_MLP(train_loader,network,optimizer,criterion):\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = 0\n",
        "        train_acc = 0\n",
        "        for data, target in train_loader:\n",
        "            data = data.view(-1,size)                                               # reshapes to 1D tensor for input\n",
        "            data, target = data.to(device), target.to(device)                       # changes both tensors to cuda\n",
        "            logits = network(data)                                                  # inputs data into network, outputs tensor as 10 output predictions as logit\n",
        "            loss = criterion(logits, target)                                        # cross entropy loss calculated (with softmax normalized probabilities) using prediction vs target\n",
        "            optimizer.zero_grad()                                                   # clears gradient to avoid accumulating old gradient (loss) values when running optimizer\n",
        "            loss.backward()                                                         # back propigates through network and updates new loss values\n",
        "            optimizer.step()                                                        # single optimization step of gradients accumulated using stochiastic gradient descent for minima\n",
        "\n",
        "            _,pred = logits.max(1)                                                  # Grab only predictions with value 1\n",
        "            num_correct = (pred==target).sum().item()                               # find number of times pred outputs match target outputs, and sum the total items\n",
        "            acc = num_correct / data.shape[0]                                       # accuracy calculation given current iteration over dataset size\n",
        "            train_acc += acc                                                        # update accuracy of training data\n",
        "\n",
        "            train_loss += loss.data                                                 # output loss found after backpropigation is accumulated\n",
        "\n",
        "        print(\n",
        "            'Train Epoch: {}  \\nLoss: {:.6f} \\nTraining Accuracy: {:.6f}'.format(\n",
        "                epoch+1, train_loss/len(train_loader), train_acc/len(train_loader))\n",
        "            )\n",
        "        test_MLP(test_loader,network)                                             # prints test accuracy at that epoch\n",
        "\n",
        "\n",
        "def train_CNN(train_loader,network,optimizer,criterion,stop):\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = 0\n",
        "        train_acc = 0\n",
        "        test_acc = 0\n",
        "        if epoch > 0:\n",
        "          test_acc = test_CNN(test_loader,network)\n",
        "          print('test accuracy: ', test_acc) ##prints previous test accuracy\n",
        "          if test_acc >= stop:\n",
        "            break\n",
        "        for data, target in train_loader:\n",
        "                                                                                    # reshapes to 1D tensor for input\n",
        "            data, target = data.to(device), target.to(device)                       # changes both tensors to cuda\n",
        "            logits = network(data)                                                  # inputs data into network, outputs tensor as 10 output predictions as logit\n",
        "            loss = criterion(logits, target)                                        # cross entropy loss calculated (with softmax normalized probabilities) using prediction vs target\n",
        "            optimizer.zero_grad()                                                   # clears gradient to avoid accumulating old gradient (loss) values when running optimizer\n",
        "            loss.backward()                                                         # back propigates through network and updates new loss values\n",
        "            optimizer.step()                                                        # single optimization step of gradients accumulated using stochiastic gradient descent for minima\n",
        "\n",
        "            _,pred = logits.max(1)                                                  # Grab only predictions with value 1\n",
        "            num_correct = (pred==target).sum().item()                               # find number of times pred outputs match target outputs, and sum the total items\n",
        "            acc = num_correct / data.shape[0]                                       # accuracy calculation given current iteration over dataset size\n",
        "            train_acc += acc                                                        # update accuracy of training data\n",
        "\n",
        "            train_loss += loss.data                                                 # output loss found after backpropigation is accumulated\n",
        "\n",
        "\n",
        "\n",
        "        print(\n",
        "            '\\nTrain Epoch: {}  \\nLoss: {:.6f} \\nTraining Accuracy: {:.6f}'.format(\n",
        "                epoch+1, train_loss/len(train_loader), train_acc/len(train_loader))\n",
        "            )\n",
        "\n",
        "\n",
        "#### 7. Evaluate the model\n",
        "\n",
        "def test_MLP(test_loader,network):\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data = data.view(-1, size)\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        logits = network(data)\n",
        "\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += pred.eq(target).float().sum().item()\n",
        "\n",
        "    total_num = len(test_loader.dataset)\n",
        "    acc = correct / total_num\n",
        "\n",
        "    return acc\n",
        "\n",
        "def test_CNN(test_loader,network):\n",
        "    correct = 0\n",
        "    network.eval()\n",
        "    for data, target in test_loader:\n",
        "\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        logits = network(data)\n",
        "\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += pred.eq(target).float().sum().item()\n",
        "\n",
        "    total_num = len(test_loader.dataset)\n",
        "    acc = correct / total_num\n",
        "\n",
        "    return acc\n",
        "\n",
        "def CNN_avgtest(tests,test_loader,network):\n",
        "  sum=0\n",
        "  tests = 20\n",
        "  for ii in range(tests):\n",
        "    sum += test_CNN(test_loader,network)\n",
        "\n",
        "  print('\\nAverage test accuracy: ', sum/tests,'\\n')\n",
        "\n",
        "\n",
        "#### 8. Make predictions\n",
        "\n",
        "def pred_CNN(test_loader, device):\n",
        "    x, y = next(iter(test_loader))\n",
        "    x, y = x.to(device), y.cuda()                                                   # note that .cuda() is original style and .to() is more dynamic\n",
        "    out = network(x)                                                                # out is the network output of testing  values\n",
        "    pred = out.argmax(dim=1)\n",
        "\n",
        "    inv_normalize = transforms.Normalize(\n",
        "        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "        std = [1/0.229, 1/0.224, 1/0.225]\n",
        "    )\n",
        "                                                  # due to softmax applied w cross entropy, the highest prob output is the prediction\n",
        "    x = inv_normalize(x)\n",
        "\n",
        "    return x,y,pred\n",
        "\n",
        "def pred_MLP(test_loader, device):\n",
        "    x, y = next(iter(test_loader))\n",
        "    x = x.view(-1, size)\n",
        "    x, y = x.to(device), y.cuda()                                                   # note that .cuda() is original style and .to() is more dynamic\n",
        "    out = network(x)                                                                # out is the network output of testing  values\n",
        "    pred = out.argmax(dim=1)\n",
        "                                                   # due to softmax applied w cross entropy, the highest prob is the prediction\n",
        "\n",
        "    return x,y,pred\n",
        "\n",
        "\n",
        "def sample_MLP(channels,height,width,img, prediction, label):\n",
        "\n",
        "    fig = plt.figure()\n",
        "    for i in range(6):\n",
        "        plt.imshow(np.transpose(img[i+10].reshape(channels,height,width),(1,2,0)))                   # required 3,32,32 shape then transposed to 32,32,3 to preserve orginal structure for plotting\n",
        "        plt.title(\"Prediction = {} Label = {}\".format(prediction[i].item(),\n",
        "                    label[i].item())\n",
        "                  )\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def sample_CNN(channels, width, height, img, prediction, label):\n",
        "\n",
        "    fig = plt.figure()\n",
        "    for i in range(6):\n",
        "        plt.imshow(np.transpose(img[i].reshape(channels,width,height),(1,2,0)))                   # required 3,32,32 shape then transposed to 32,32,3 to preserve orginal structure for plotting\n",
        "        plt.title(\"Prediction = {} Label = {}\".format(prediction[i].item(),\n",
        "                    label[i].item())\n",
        "                  )\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" --------------------------- PROGRAM --------------------------- \"\"\"\n",
        "### STICKING WITH leakyReLU for MLP DUE TO DUE TO LESS DOWNSIDES THAN TANH & SIG, and MORE RELIABLE THAN LEAKY\n",
        "### ALSO ReLU IS FASTER THAN LOGISTIC REGRESSIONS LIKE TANH AND SIGMOID\n",
        "### STOCHIASTIC GRADIENT DESCENT (SGD) IS USED IN CONFIGURE\n",
        "### SGD WORKED BETTER THAN ADAM WITH AMSGRAD\n",
        "\n",
        "#### BASIC SETTINGS\n",
        "device = torch.device('cuda:0')                                                   # note that to.(device) is better than .cuda() due to dynamic input\n",
        "\n",
        "img_width = 224\n",
        "img_height = img_width\n",
        "channels = 3\n",
        "\n",
        "batch_size = 16               #80 imgs per flower (73 train, 7 test)    # small size used for CNN due to small validation set\n",
        "learning_rate = .0001                                                              # step size. Lower step size for CNN w momentum helps\n",
        "epochs = 300\n",
        "outputs = 6\n",
        "size = img_height*img_width*channels\n",
        "neurons = size                                                                    # a neural network that is equal to or greater than the number of inputs 'should' be more accurate\n",
        "momentum = 0.9\n",
        "\n",
        "rotation = 30\n",
        "resize = img_width\n",
        "norm_param = (0.485,0.456,0.406),(0.229,0.224,0.225) # mean and standard deviation of normalized values of each channel (3,RGB)\n",
        "\n",
        "stop_percent = 0.965 ##overrides epochs to get highest observed test score. Comment out if you dont know the tests potential\n",
        "\n",
        "\n",
        "#### INIT DATA\n",
        "train_dir = 'flowers6/train'\n",
        "test_dir = 'flowers6/test'\n",
        "train_data, test_data = transform(train_dir,test_dir,rotation,resize,norm_param)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle=True)\n",
        "\n",
        "#### INIT MODEL PARAMETERS\n",
        "network   = CNN().to(device)                                                 # MLP classifier class selected for neural network algorithm\n",
        "optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=0.9)                # stochastic gradient descent is selected for gradient analysis\n",
        "criterion = nn.CrossEntropyLoss().to(device)                                 # cross entropy is selected for loss (aka cost) function\n",
        "\n",
        "#### PRINT SETTINGS\n",
        "print('\\nSettings\\nbatch_size: ',batch_size, '\\nlearning_rate:',learning_rate, '\\nmax epochs: ',epochs, '\\nmomentum: ', momentum)\n",
        "\n",
        "#### TRAIN MODEL\n",
        "train_CNN(train_loader,network,optimizer,criterion,stop_percent)\n",
        "\n",
        "#### TEST MODEL\n",
        "tests=1\n",
        "CNN_avgtest(tests,test_loader,network)\n",
        "\n",
        "#### EVAL OF RANDOM INPUT\n",
        "x,y,pred = pred_CNN(test_loader,device)\n",
        "sample_CNN(channels, img_height, img_width, x.detach().cpu().numpy(), pred, y) #img, prediction, label\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e4Bt48RBit4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for i, batch in enumerate(train_loader):\n",
        "#    print(len(batch[0]))\n",
        "batch = enumerate(train_loader)\n",
        "print(batch)\n",
        "#train_loader.item(0)\n",
        "train_np = np.asarray(train_loader.dataset,dtype=object)\n",
        "train_np.size\n",
        "len(train_loader.dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqgw2pc5h6Gc",
        "outputId": "3e6b8264-0742-4491-f820-ee13cd71423a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<enumerate object at 0x7f073c0035f0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "438"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yZ2RI5apJZOn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca1f6fef-c50d-473d-ee9d-12d56469303c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12.514285714285714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip flowers6.zip"
      ],
      "metadata": {
        "id": "NfFPOMJo5uk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3880ad60-a47b-4254-f222-fb447b1cb55d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  flowers6.zip\n",
            "   creating: flowers6/\n",
            "   creating: flowers6/test/\n",
            "   creating: flowers6/test/buttercup/\n",
            "  inflating: flowers6/test/buttercup/image_1194.jpg  \n",
            "  inflating: flowers6/test/buttercup/image_1195.jpg  \n",
            "  inflating: flowers6/test/buttercup/image_1196.jpg  \n",
            "  inflating: flowers6/test/buttercup/image_1197.jpg  \n",
            "  inflating: flowers6/test/buttercup/image_1198.jpg  \n",
            "  inflating: flowers6/test/buttercup/image_1199.jpg  \n",
            "  inflating: flowers6/test/buttercup/image_1200.jpg  \n",
            "   creating: flowers6/test/daisy/\n",
            "  inflating: flowers6/test/daisy/image_0874.jpg  \n",
            "  inflating: flowers6/test/daisy/image_0875.jpg  \n",
            "  inflating: flowers6/test/daisy/image_0876.jpg  \n",
            "  inflating: flowers6/test/daisy/image_0877.jpg  \n",
            "  inflating: flowers6/test/daisy/image_0878.jpg  \n",
            "  inflating: flowers6/test/daisy/image_0879.jpg  \n",
            "  inflating: flowers6/test/daisy/image_0880.jpg  \n",
            "   creating: flowers6/test/fritillary/\n",
            "  inflating: flowers6/test/fritillary/image_0714.jpg  \n",
            "  inflating: flowers6/test/fritillary/image_0715.jpg  \n",
            "  inflating: flowers6/test/fritillary/image_0716.jpg  \n",
            "  inflating: flowers6/test/fritillary/image_0717.jpg  \n",
            "  inflating: flowers6/test/fritillary/image_0718.jpg  \n",
            "  inflating: flowers6/test/fritillary/image_0719.jpg  \n",
            "  inflating: flowers6/test/fritillary/image_0720.jpg  \n",
            "   creating: flowers6/test/snowdrop/\n",
            "  inflating: flowers6/test/snowdrop/image_0154.jpg  \n",
            "  inflating: flowers6/test/snowdrop/image_0155.jpg  \n",
            "  inflating: flowers6/test/snowdrop/image_0156.jpg  \n",
            "  inflating: flowers6/test/snowdrop/image_0157.jpg  \n",
            "  inflating: flowers6/test/snowdrop/image_0158.jpg  \n",
            "  inflating: flowers6/test/snowdrop/image_0159.jpg  \n",
            "  inflating: flowers6/test/snowdrop/image_0160.jpg  \n",
            "   creating: flowers6/test/sunflower/\n",
            "  inflating: flowers6/test/sunflower/image_0794.jpg  \n",
            "  inflating: flowers6/test/sunflower/image_0795.jpg  \n",
            "  inflating: flowers6/test/sunflower/image_0796.jpg  \n",
            "  inflating: flowers6/test/sunflower/image_0797.jpg  \n",
            "  inflating: flowers6/test/sunflower/image_0798.jpg  \n",
            "  inflating: flowers6/test/sunflower/image_0799.jpg  \n",
            "  inflating: flowers6/test/sunflower/image_0800.jpg  \n",
            "   creating: flowers6/test/windflower/\n",
            "  inflating: flowers6/test/windflower/image_1274.jpg  \n",
            "  inflating: flowers6/test/windflower/image_1275.jpg  \n",
            "  inflating: flowers6/test/windflower/image_1276.jpg  \n",
            "  inflating: flowers6/test/windflower/image_1277.jpg  \n",
            "  inflating: flowers6/test/windflower/image_1278.jpg  \n",
            "  inflating: flowers6/test/windflower/image_1279.jpg  \n",
            "  inflating: flowers6/test/windflower/image_1280.jpg  \n",
            "   creating: flowers6/train/\n",
            "   creating: flowers6/train/buttercup/\n",
            "  inflating: flowers6/train/buttercup/image_1121.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1122.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1123.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1124.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1125.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1126.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1127.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1128.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1129.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1130.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1131.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1132.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1133.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1134.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1135.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1136.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1137.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1138.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1139.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1140.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1141.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1142.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1143.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1144.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1145.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1146.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1147.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1148.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1149.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1150.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1151.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1152.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1153.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1154.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1155.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1156.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1157.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1158.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1159.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1160.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1161.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1162.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1163.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1164.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1165.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1166.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1167.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1168.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1169.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1170.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1171.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1172.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1173.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1174.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1175.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1176.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1177.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1178.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1179.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1180.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1181.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1182.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1183.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1184.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1185.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1186.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1187.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1188.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1189.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1190.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1191.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1192.jpg  \n",
            "  inflating: flowers6/train/buttercup/image_1193.jpg  \n",
            "   creating: flowers6/train/daisy/\n",
            "  inflating: flowers6/train/daisy/image_0801.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0802.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0803.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0804.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0805.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0806.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0807.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0808.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0809.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0810.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0811.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0812.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0813.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0814.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0815.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0816.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0817.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0818.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0819.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0820.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0821.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0822.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0823.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0824.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0825.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0826.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0827.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0828.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0829.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0830.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0831.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0832.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0833.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0834.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0835.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0836.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0837.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0838.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0839.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0840.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0841.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0842.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0843.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0844.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0845.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0846.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0847.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0848.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0849.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0850.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0851.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0852.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0853.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0854.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0855.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0856.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0857.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0858.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0859.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0860.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0861.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0862.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0863.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0864.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0865.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0866.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0867.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0868.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0869.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0870.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0871.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0872.jpg  \n",
            "  inflating: flowers6/train/daisy/image_0873.jpg  \n",
            "   creating: flowers6/train/fritillary/\n",
            "  inflating: flowers6/train/fritillary/image_0641.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0642.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0643.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0644.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0645.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0646.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0647.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0648.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0649.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0650.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0651.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0652.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0653.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0654.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0655.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0656.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0657.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0658.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0659.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0660.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0661.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0662.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0663.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0664.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0665.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0666.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0667.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0668.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0669.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0670.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0671.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0672.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0673.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0674.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0675.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0676.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0677.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0678.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0679.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0680.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0681.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0682.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0683.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0684.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0685.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0686.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0687.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0688.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0689.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0690.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0691.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0692.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0693.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0694.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0695.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0696.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0697.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0698.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0699.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0700.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0701.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0702.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0703.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0704.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0705.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0706.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0707.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0708.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0709.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0710.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0711.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0712.jpg  \n",
            "  inflating: flowers6/train/fritillary/image_0713.jpg  \n",
            "   creating: flowers6/train/snowdrop/\n",
            "  inflating: flowers6/train/snowdrop/image_0081.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0082.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0083.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0084.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0085.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0086.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0087.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0088.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0089.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0090.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0091.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0092.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0093.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0094.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0095.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0096.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0097.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0098.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0099.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0100.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0101.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0102.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0103.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0104.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0105.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0106.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0107.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0108.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0109.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0110.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0111.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0112.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0113.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0114.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0115.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0116.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0117.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0118.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0119.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0120.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0121.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0122.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0123.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0124.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0125.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0126.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0127.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0128.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0129.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0130.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0131.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0132.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0133.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0134.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0135.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0136.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0137.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0138.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0139.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0140.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0141.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0142.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0143.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0144.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0145.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0146.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0147.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0148.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0149.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0150.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0151.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0152.jpg  \n",
            "  inflating: flowers6/train/snowdrop/image_0153.jpg  \n",
            "   creating: flowers6/train/sunflower/\n",
            "  inflating: flowers6/train/sunflower/image_0721.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0722.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0723.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0724.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0725.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0726.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0727.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0728.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0729.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0730.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0731.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0732.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0733.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0734.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0735.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0736.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0737.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0738.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0739.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0740.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0741.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0742.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0743.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0744.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0745.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0746.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0747.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0748.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0749.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0750.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0751.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0752.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0753.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0754.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0755.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0756.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0757.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0758.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0759.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0760.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0761.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0762.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0763.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0764.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0765.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0766.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0767.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0768.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0769.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0770.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0771.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0772.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0773.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0774.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0775.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0776.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0777.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0778.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0779.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0780.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0781.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0782.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0783.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0784.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0785.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0786.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0787.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0788.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0789.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0790.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0791.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0792.jpg  \n",
            "  inflating: flowers6/train/sunflower/image_0793.jpg  \n",
            "   creating: flowers6/train/windflower/\n",
            "  inflating: flowers6/train/windflower/image_1201.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1202.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1203.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1204.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1205.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1206.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1207.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1208.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1209.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1210.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1211.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1212.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1213.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1214.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1215.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1216.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1217.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1218.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1219.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1220.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1221.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1222.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1223.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1224.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1225.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1226.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1227.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1228.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1229.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1230.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1231.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1232.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1233.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1234.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1235.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1236.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1237.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1238.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1239.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1240.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1241.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1242.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1243.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1244.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1245.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1246.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1247.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1248.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1249.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1250.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1251.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1252.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1253.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1254.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1255.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1256.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1257.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1258.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1259.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1260.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1261.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1262.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1263.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1264.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1265.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1266.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1267.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1268.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1269.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1270.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1271.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1272.jpg  \n",
            "  inflating: flowers6/train/windflower/image_1273.jpg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def imshow(img):\n",
        "    #img = img / 2 + 0.5     # unnormalize\n",
        "    inv_normalize = transforms.Normalize(\n",
        "        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "        std = [1/0.229, 1/0.224, 1/0.225])\n",
        "    img = inv_normalize(img)\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# get random images\n",
        "images, labels = iter(train_loader).next()\n",
        "\n",
        "\n",
        "## get specific images\n",
        "#images, labels = train_loader\n",
        "#for i in range(images.shape[0]):\n",
        "#images[i] = images[i]\n",
        "\n",
        "\n",
        "# show images\n",
        "imshow(images[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "3PEJ5YCrQr_b",
        "outputId": "7e84691c-2a96-4ade-a8cf-aaf90cafecc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-4eb2c4347add>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# get random images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
          ]
        }
      ]
    }
  ]
}